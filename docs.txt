1. Introduction (30–45 sec)

👤 You on camera (optional) or just voice over:

“Hello everyone, my name is Shivam Rawat, and today I’ll be presenting my project — a chatbot built using LangGraph, Groq LLM, Tavily Search, and Streamlit. The idea is to create a chatbot that can intelligently answer questions, and if it doesn’t know something, it can call a web search tool to fetch real-time information.”

2. Tech Stack (30 sec)

💻 Show a slide or quickly explain with your code open:

“The tech stack includes:

LangGraph, which manages the flow between the agent and tools.

Groq’s LLM – openai/gpt-oss-20b, which generates the chatbot’s responses.

Tavily Search API, which fetches live search results from the web.

And finally, Streamlit, which I used to build a simple user interface.”

3. Workflow Explanation (1 min)

📊 Show a diagram or explain while pointing at code:

“The workflow works like this:

The user types a message into the chatbot interface.

The message goes to the Agent (Groq LLM).

If the model detects that it needs real-time info, it makes a tool call to Tavily Search.

Tavily fetches results and sends them back.

The Agent uses those results to generate a final answer.

Finally, the answer is displayed in Streamlit.”

4. Live Demo (2–3 min)

💻 Switch to your Streamlit app and show live examples:

“Let’s see the chatbot in action.

First, I’ll ask a simple question: ‘What is the capital of France?’ — here, the LLM directly knows the answer.

Next, I’ll ask something that needs real-time information: ‘What is the latest news about SpaceX?’ — now the model decides to call Tavily Search, fetches recent updates, and then gives me a summarized answer.

Finally, let me try a casual question: ‘Tell me a fun fact about cats.’ — this shows how it can also respond conversationally.”

5. Code Walkthrough (2–3 min)

👨‍💻 Switch to your IDE (VS Code or wherever you wrote code):

“Now, let me quickly walk you through the code.

Inside the chatbot class, I define the LLM and bind it to the tools.

The call_model function sends messages to the LLM.

The router_function decides whether to end the conversation or call a tool.

Using StateGraph from LangGraph, I connect the nodes: agent, tools, start, and end.

Finally, in the Streamlit app, I created a simple interface with an input box, a button, and a display area for responses.”

6. Conclusion & Future Scope (30–45 sec)

📌 End with vision:

“In conclusion, this chatbot shows how LangGraph can orchestrate tools and language models together. With Groq’s LLM for fast inference, Tavily for real-time search, and Streamlit for an interactive UI, this project demonstrates a modular and scalable chatbot framework.

In the future, I could extend it by adding memory for longer conversations, integrating multiple tools like weather or translation, and deploying it as a web app for public use.”

“Thank you for watching this demo!”